Changed directory to /home/grads/r/rishit/code/Pyfr_venv/PyFR/examples/euler_vortex_2d.

JobID: 59203
======
Time: Mon Nov  8 16:44:59 CST 2021
Running on master node: spitfire-ng21.spitfire.engr.tamu.edu
Current directory: /home/grads/r/rishit/code/Pyfr_venv/PyFR/examples/euler_vortex_2d

numtasks=4, numnodes=2, mpi_tasks_per_node=2 (OMP_NUM_THREADS=)

Executing command:
==================
mpiexec --mca btl_openib_allow_ib 1 -n 4  /home/grads/r/rishit/code/Pyfr_venv/PyFR/pyfr/pyfr run -b hip -p euler_vortex_2d.pyfrm euler_vortex_2d.ini

--------------------------------------------------------------------------
The library attempted to open the following supporting CUDA libraries,
but each of them failed.  CUDA-aware support is disabled.
libcuda.so.1: cannot open shared object file: No such file or directory
libcuda.dylib: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.so.1: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.dylib: cannot open shared object file: No such file or directory
If you are not interested in CUDA-aware support, then run with
--mca opal_warn_on_missing_libcuda 0 to suppress this message.  If you are interested
in CUDA-aware support, then try setting LD_LIBRARY_PATH to the location
of libcuda.so.1 to get passed this issue.
--------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/grads/r/rishit/code/Pyfr_venv/PyFR/pyfr/pyfr", line 273, in <module>
    main()
  File "/home/grads/r/rishit/code/Pyfr_venv/PyFR/pyfr/pyfr", line 117, in main
    args.process(args)
  File "/home/grads/r/rishit/code/Pyfr_venv/PyFR/pyfr/pyfr", line 250, in process_run
    _process_common(
  File "/home/grads/r/rishit/code/Pyfr_venv/PyFR/pyfr/pyfr", line 226, in _process_common
    backend = get_backend(args.backend, cfg)
  File "/home/grads/r/rishit/code/Pyfr_venv/PyFR/pyfr/backends/__init__.py", line 12, in get_backend
    return subclass_where(BaseBackend, name=name.lower())(cfg)
  File "/home/grads/r/rishit/code/Pyfr_venv/PyFR/pyfr/backends/hip/base.py", line 33, in __init__
    self.hip.set_device(int(devid))
  File "/home/grads/r/rishit/code/Pyfr_venv/PyFR/pyfr/backends/hip/driver.py", line 266, in set_device
Traceback (most recent call last):
  File "/home/grads/r/rishit/code/Pyfr_venv/PyFR/pyfr/pyfr", line 273, in <module>
    self.lib.hipSetDevice(devid)
  File "/home/grads/r/rishit/code/Pyfr_venv/PyFR/pyfr/ctypesutil.py", line 33, in _errcheck
    raise self._statuses[status]
    main()
  File "/home/grads/r/rishit/code/Pyfr_venv/PyFR/pyfr/pyfr", line 117, in main
pyfr.backends.hip.driver.HIPInvalidDevice
    args.process(args)
  File "/home/grads/r/rishit/code/Pyfr_venv/PyFR/pyfr/pyfr", line 250, in process_run
    _process_common(
  File "/home/grads/r/rishit/code/Pyfr_venv/PyFR/pyfr/pyfr", line 226, in _process_common
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 3 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    backend = get_backend(args.backend, cfg)
  File "/home/grads/r/rishit/code/Pyfr_venv/PyFR/pyfr/backends/__init__.py", line 12, in get_backend
    return subclass_where(BaseBackend, name=name.lower())(cfg)
  File "/home/grads/r/rishit/code/Pyfr_venv/PyFR/pyfr/backends/hip/base.py", line 33, in __init__
    self.hip.set_device(int(devid))
  File "/home/grads/r/rishit/code/Pyfr_venv/PyFR/pyfr/backends/hip/driver.py", line 266, in set_device
[spitfire-ng21.spitfire.engr.tamu.edu:47509] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198
    self.lib.hipSetDevice(devid)
  File "/home/grads/r/rishit/code/Pyfr_venv/PyFR/pyfr/ctypesutil.py", line 33, in _errcheck
    raise self._statuses[status]
pyfr.backends.hip.driver.HIPInvalidDevice
/home/grads/r/rishit/code/Pyfr_venv/lib/python3.9/site-packages/pytools-2021.2.8-py3.9.egg/pytools/prefork.py:94: UserWarning: Prefork server exiting upon apparent death of parent
  warn(f"{who} exiting upon apparent death of {partner}")
[spitfire-ng21.spitfire.engr.tamu.edu:47509] 3 more processes have sent help message help-mpi-common-cuda.txt / dlopen failed
[spitfire-ng21.spitfire.engr.tamu.edu:47509] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[spitfire-ng21.spitfire.engr.tamu.edu:47509] 1 more process has sent help message help-mpi-api.txt / mpi-abort
